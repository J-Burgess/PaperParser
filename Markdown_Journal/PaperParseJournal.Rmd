---
title: "JB230518_PaperParser_Journal"
author: "JBurgess"
date: "2023-05-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# JB230518 Initializing repository of PaperParser
* A project to summarize a list of given input papers using an LLM and output a searchable vector memory. 

## Installing required packages:
```{bash, eval=F}
conda create -n pparser

mamba install -c conda-forge openai
pip install langchain
pip install chromadb
pip install PyPDF2
pip install unstructured

pip install tiktoken
```

# JB230610 Installing llama.cpp to locally embed papers.
* As the majority of expense occured to OpenAI embeddings use I would like to explore the feasability of locally embedding documents. Querying should be cheap enough to continue using the OpenAI API. 
* Langchain supports llama.cpp wrappings:
  + https://python.langchain.com/en/latest/reference/modules/embeddings.html
* Cloned the llama.cpp repository:
  + https://github.com/ggerganov/llama.cpp

```{bash, eval=F}
conda activate pparser
pip install llama-cpp-python
#Moved llama 7B pytorch weights into /models/7B 

#Created an environment to run the model conversion and quantization. 
conda activate llama
python3 -m pip install -r requirements.txt #in the root dir of llama.cpp repo
#Downloaded tokenizer.model and tokenizer_config.json from here.
https://huggingface.co/chavinlo/gpt4-x-alpaca/tree/main

# convert the 7B model to ggml FP16 format
python3 convert.py models/7B/
# quantize the model to 4-bits (using q4_0 method)
./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0

# run the inference
./main -m ./models/7B/ggml-model-q4_0.bin -n 128
```

## Conclusions:
* The 7B quantized model works fine to embed the 2 papers as previously done with OpenAI api. However, the first query doesnt seem to work at all and it picks the source as the wrong paper. 
* Could be due to less complex models being unable to extract higher complexity patterns (poorer quality embeddings.)

* Lets try with the large InstructorEmbeddings model that can run on cuda gpu. 
```{bash,eval=F}
conda activate pparser
pip install InstructorEmbedding
pip install -U sentence-transformers

```

## Conclusions: Instructor large local model works fantastic. 
* cuda accelerated embedding using huggingface intstructor large model completed embedding the 2 papers very fast and was able to retrieve relevant results from the VectorDB. This will be the one I use from now on!


* Next steps:
  + Iteratively add to the database rather than load all pdf chunks into memory.
  + API access to arxiv (already exists) or BioRxiv.
  
# JB230611 Iteratively adding completed.
* Iterative adding to the vectordb has been implemented. 
* Before adding an arxiv API access maybe I can try a program to chat with your database? Local hosted website app or something? 


# JB230713 Containerizing project:
```{bash, eval=F}
singularity build --fakeroot ../NeuroVecta.sif NeuroVecta.def

```


